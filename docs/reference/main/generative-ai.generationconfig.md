<!-- Do not edit this file. It is automatically generated by API Documenter. -->

[Home](./index.md) &gt; [@google/generative-ai](./generative-ai.md) &gt; [GenerationConfig](./generative-ai.generationconfig.md)

## GenerationConfig interface

Config options for content-related requests

**Signature:**

```typescript
export interface GenerationConfig 
```

## Properties

<table><thead><tr><th>

Property


</th><th>

Modifiers


</th><th>

Type


</th><th>

Description


</th></tr></thead>
<tbody><tr><td>

[candidateCount?](./generative-ai.generationconfig.candidatecount.md)


</td><td>


</td><td>

number


</td><td>

_(Optional)_


</td></tr>
<tr><td>

[frequencyPenalty?](./generative-ai.generationconfig.frequencypenalty.md)


</td><td>


</td><td>

number


</td><td>

_(Optional)_ Frequency penalty applied to the next token's logprobs, multiplied by the number of times each token has been seen in the respponse so far.


</td></tr>
<tr><td>

[logprobs?](./generative-ai.generationconfig.logprobs.md)


</td><td>


</td><td>

number


</td><td>

_(Optional)_ Valid if responseLogProbs is set to True. This will set the number of top logprobs to return at each decoding step in the logprobsResult.


</td></tr>
<tr><td>

[maxOutputTokens?](./generative-ai.generationconfig.maxoutputtokens.md)


</td><td>


</td><td>

number


</td><td>

_(Optional)_


</td></tr>
<tr><td>

[presencePenalty?](./generative-ai.generationconfig.presencepenalty.md)


</td><td>


</td><td>

number


</td><td>

_(Optional)_ Presence penalty applied to the next token's logprobs if the token has already been seen in the response.


</td></tr>
<tr><td>

[responseLogprobs?](./generative-ai.generationconfig.responselogprobs.md)


</td><td>


</td><td>

boolean


</td><td>

_(Optional)_ If True, export the logprobs results in response.


</td></tr>
<tr><td>

[responseMimeType?](./generative-ai.generationconfig.responsemimetype.md)


</td><td>


</td><td>

string


</td><td>

_(Optional)_ Output response mimetype of the generated candidate text. Supported mimetype: `text/plain`<!-- -->: (default) Text output. `application/json`<!-- -->: JSON response in the candidates.


</td></tr>
<tr><td>

[responseSchema?](./generative-ai.generationconfig.responseschema.md)


</td><td>


</td><td>

[ResponseSchema](./generative-ai.responseschema.md)


</td><td>

_(Optional)_ Output response schema of the generated candidate text. Note: This only applies when the specified `responseMIMEType` supports a schema; currently this is limited to `application/json`<!-- -->.


</td></tr>
<tr><td>

[stopSequences?](./generative-ai.generationconfig.stopsequences.md)


</td><td>


</td><td>

string\[\]


</td><td>

_(Optional)_


</td></tr>
<tr><td>

[temperature?](./generative-ai.generationconfig.temperature.md)


</td><td>


</td><td>

number


</td><td>

_(Optional)_


</td></tr>
<tr><td>

[topK?](./generative-ai.generationconfig.topk.md)


</td><td>


</td><td>

number


</td><td>

_(Optional)_


</td></tr>
<tr><td>

[topP?](./generative-ai.generationconfig.topp.md)


</td><td>


</td><td>

number


</td><td>

_(Optional)_


</td></tr>
</tbody></table>
